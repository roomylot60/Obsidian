### Embedding

#### torch.nn.Embedding(num_embeddings, embedding_dim)
- `num_embeddings`과 `embedding_dim`을 정하는 기준은 다양한 요소와 상황에 따라 결정
1. **데이터의 크기와 복잡성**
    - `num_embeddings`*(int)*: 사용하려는 고유한 항목(예: 어휘의 크기, 카테고리의 수)의 총 개수입니다. 이 값은 데이터셋의 다양성과 복잡성에 따라 달라집니다.
    - `embedding_dim`*(int)*: 임베딩 벡터의 차원입니다. 일반적으로 이 값은 크게 설정될수록 모델의 표현 능력이 증가하지만, 계산 비용이 높아질 수 있습니다.
2. **컴퓨팅 리소스**
    - `embedding_dim`이 높을수록 연산량이 증가하므로, 사용 가능한 컴퓨팅 리소스를 고려하여 적절한 값을 선택해야 합니다.
3. **모델의 성능과 복잡성**
    - 임베딩 벡터의 차원이 클수록 모델은 더 복잡한 패턴과 관계를 학습할 수 있습니다. 하지만, 이는 과적합의 위험도 증가시킬 수 있으므로 조심스럽게 설정해야 합니다.
4. **이전 연구나 벤치마크**
    - 유사한 작업이나 데이터셋에서 얻은 성과를 참고하여 `num_embeddings`과 `embedding_dim`의 초기 값을 설정할 수 있습니다.
5. **하이퍼파라미터 튜닝**
    - 초기 모델 학습 후 성능을 평가하고, 필요한 경우 `num_embeddings`과 `embedding_dim`을 조절하여 모델의 성능을 향상시킬 수 있습니다.

---
### Word2Vec

- Word2Vec은 자연어 처리(NLP)와 텍스트 마이닝 분야에서 널리 사용되는 word embedding 기법
- 이 기법은 단어를 고차원 벡터로 표현하여 단어 간의 의미적 유사성을 캡처
- Word2Vec 모델은 대량의 텍스트 데이터에서 단어의 분산 표현(distributed representation)을 학습하는 데 효과적입니다.

- **의미적 유사성 캡처**: Word2Vec은 단어 간의 의미적 유사성을 캡처하는 데 효과적입니다. 예를 들어, 유사한 의미를 가진 단어들이 벡터 공간에서 서로 가까이 위치하게 됩니다.
- **효율적인 벡터 표현**: Word2Vec은 각 단어를 고정된 차원의 밀집 벡터로 표현하므로, 메모리 사용량이 적고 계산이 빠릅니다.
- **전이 학습**: Word2Vec은 사전 훈련된 임베딩을 사용하여 다양한 자연어 처리 작업에 전이 학습(transfer learning)을 적용할 수 있습니다.
#### Skip-gram
- 주어진 단어로 주변의 단어를 예측하는 방식입니다.
- 예를 들어, "apple"이라는 단어가 주어졌을 때 "juicy", "red", "fruit" 등의 주변 단어를 예측하는 방식입니다.
#### CBOW (Continuous Bag of Words)
- 주변 단어들을 통해 중앙의 단어를 예측하는 방식입니다.
- 예를 들어, "juicy", "red", "fruit"과 같은 주변 단어들이 주어졌을 때 "apple"이라는 중앙 단어를 예측하는 방식입니다.
