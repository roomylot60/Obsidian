# Mapping the Mind of a Large Language Model

Link : https://www.anthropic.com/research/mapping-mind-language-model
Paper link : https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html

---

오늘날 AI 모델의 내부 동작에 대한 이해에 있어서 많은 발전이 있었다.
우리는 우리가 제공하는 LLM 중 하나인 클로드 소넷의 내부를 설명하는 수 많은 컨셉에 대해 구분했다.
최초로 현대적인 production-grade LLM의 내부를 세부화한 것 이다. 
해석능력 발견 클라우드(interpretability discovery cloud)는 앞으로 우리의 AI 모델을 더욱 안전하게 해줄 것이다.

우리는 대부분 AI 모델을 블랙박스로 다룬다 : 어떠한 입력에 대한 출력이 있고, 왜 모델이 그러한 특정 반응을 출력하는 지에 대해서는 분명하지 않다.
이러한 점이 모델의 안전성에 대해 확신하기 어렵게 한다 : 만약 우리가 동작 원리를 모르면, 그것이 나쁘거나, 편향적이거나, 신뢰도가 낮거나, 위험한 결과임을 어찌 알겠는가
어떻게 우리가 그것이 안전하고 믿을만한지 알 수 있겠는가

블랙박스 내부를 열어보는 것이 반드시 도움이 되지는 않는다 : 모델의 내부 상태, 그 모델이 결과를 작성하기 전에 "생각하는 것"은 명확한 의미를 가지지 않은 숫자들로 구성된 긴 리스트("뉴런 활성화")로 만들어져 있다.
클로드와 같은 모델과의 상호작용으로부터 넓은 범위의 컨셉을 이해하고 사용할 수 있다. 그러나 뉴런에서 직접적으로 보아 구분할 수는 없다.
이는 각각의 컨셉이 많은 뉴런을 통해 표현되고, 각각의 뉴런은 많은 컨셉들의 표현에 포함되는 것으로 알 수 있다.

이전에 우리는 특성이라 불리는 뉴런 활성의 패턴을 대조하는 것에서 인간이 이해할 수 있는 개념으로 약간의 진전을 이루었다.
우리는 클래식한 ML로부터 따와서 많은 다른 문맥에서 재발하는 뉴런 활성의 패턴을 고립시키는 "사전 학습;dictionary learning"이라 불리는 기술을 사용했다. 
차례로 모델의 어떠한 내부 상태는 많은 활성 뉴런 대신  몇 가지 특성의 측면에서 표현가능했다.
사전에 있는 모든 영단어가 문자들의 결합을 통해 만들어지고, 문장은 단어들로 구성되는 것처럼, AI 모델에서의 모든 특성은 뉴런의 결합을 통해 만들어지고 모든 내부 상태는 특성의 결합으로 구성된다.

2023년 10월, 사전 학습을 매우 작은 "toy" LM에 적용하는 데 성공했고, 대문자 문장, DNA 시퀀스, 인증서에서 성씨들, 수학에서의 명사들, 파이썬 코드에서 함수 설명문 등과 같은 컨셉에서 작용하는 일관성있는 특성들을 발견했다.

이러한 컨셉은 흥미로우나, 모델 자체는 매우 간단하다. 다른 연구자들이 후에 그들의 기존 연구에 비해 다소 더 거대하고 복잡한 모델들과 유사한 기술들이라고 언급했다.
그러나 우리는 지금 사용 중인 일반 규모보다 광대하고 더욱 큰 AI LM에 대해 기술 규모를 크게 할 수 있을 것이라는 데 긍정적이다. 또한 그리함으로써, 세세한 동작들을 지원하는 수많은 특성에 대해 학습할 수 있을 것이다.
거대 규모의 많은 시도를 통해 이루어질 것이다. 

---

1. 하나의 모델을 통해 수많은 특성(feature)들을 추출할 수 있다.
2. 특성들은 해석 가능하고(interpretable) 어떠한 의미를 가지며, 대부분 안정성과 관련이 있다.
3. 특성들은 분류 작업에 유용하고, 모델의 동작을 조정(steer)할 수 있다.