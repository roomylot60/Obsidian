## VLLM 성능 분석(Benchmark)

### 1. 시스템 스펙 기록
GPU, CPU, OS, Driver, CUDA, vLLM 버전, Model 등

### 2. vLLM 설정 기록
- `--tensor-parallel-size`
- `--quantization`
- `--gpu-memory-utilization`
- `--max-model-len`

### 3. Benchmark Senario
- 요약 기반 분석
    - 1. 짧은 입력(1k tokens)
    - 2. 중간 입력(3k tokens)
    - 3. 긴 입력(6k tokens)
    - 4. 매우 긴 입력(10k tokens)
    - 5. 동시 요청 처리 성능(concurrency=1, 2, 4, 8)

### 4. 측정 지표
- 응답 시간: prefill 시간, decode 시간
- 처리량(throughout)
- 토큰 생성 속도
- GPU 메모리(VRAM) 사용량
- OOM/에러 발생 여부
- `--chunked-prefill`ON/OFF 비교
- `--max-num-batched-tokens`에 따른 변화
- `--tensor-parallel-size`에 따른 변화

# 벤치마크용 기본 스크립트(HTTP OpenAI API 방식)
- latency, tokens/sec, throughput 측정 가능
```python
import time
import requests

API_URL = "http://localhost:8000/v1/chat/completions"
MODEL = "llama"

def run_test(prompt, name="test"):
    payload = {
        "model": MODEL,
        "messages": [{"role":"user","content": prompt}],
        "max_tokens": 512
    }
    start = time.time()
    resp = requests.post(API_URL, json=payload, timeout=300)
    end = time.time()

    result = resp.json()

    total_tokens = result["usage"]["total_tokens"]
    gen_tokens = result["usage"]["completion_tokens"]

    latency = end - start
    tps = gen_tokens / latency

    return {
        "case": name,
        "latency_sec": round(latency, 4),
        "total_tokens": total_tokens,
        "generated_tokens": gen_tokens,
        "tokens_per_sec": round(tps, 4)
    }


def generate_prompt(token_count):
    return "안녕하세요. 이것은 성능 측정용 입력입니다.\n" * (token_count // 10)

tests = [
    ("short_1k", generate_prompt(1000)),
    ("mid_3k", generate_prompt(3000)),
    ("long_6k", generate_prompt(6000)),
    ("verylong_10k", generate_prompt(10000)),
]

for name, prompt in tests:
    result = run_test(prompt, name)
    print(result)
```

## Chunked Prefill ON/OFF 비교 템플릿
- 두 개의 vLLM 서버를 띄워서 비교하는 방식이 가장 명확

## 동시성(concurrency) 테스트 템플릿
- Locust / wrk 대신 Python 병렬 요청 템플릿 제공
```python
import time
import requests
from concurrent.futures import ThreadPoolExecutor

API_URL = "http://localhost:8000/v1/chat/completions"
MODEL = "llama"

def call_api(prompt):
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 300
    }
    start = time.time()
    resp = requests.post(API_URL, json=payload)
    end = time.time()
    return end - start


def run_concurrency_test(c):
    prompt = "안녕하세요. 긴 문서 요약 성능 측정 중입니다." * 500
    with ThreadPoolExecutor(max_workers=c) as exe:
        results = list(exe.map(call_api, [prompt]*c))
    return {
        "concurrency": c,
        "avg_latency": sum(results)/len(results),
        "max_latency": max(results),
        "min_latency": min(results)
    }


for c in [1, 2, 4, 8]:
    print(run_concurrency_test(c))
```

## GPU 메모리 측정 템플릿
- `nvidia-smi` 모니터링
```bash
watch -n 0.5 nvidia-smi
```

- vLLM 메트릭 서버 활성화 후 프로메테우스 스크랩
```powershell
--metrics-port 8080
```

## 성능 분석 결과 리포트 템플릿
### vLLM 성능 분석 리포트
1. 테스트 환경

CPU:
GPU:
VRAM:
GPU 개수:
OS:
CUDA:
드라이버:
vLLM 버전:
모델:
Tensor Parallel:
Quantization:
Chunked Prefill: ON/OFF

2. 벤치마크 결과 요약

| 케이스       | 입력 길이 | Latency | Tokens/sec | GPU VRAM | 비고 |
| --------- | ----- | ------- | ---------- | -------- | -- |
| short     | 1k    |         |            |          |    |
| mid       | 3k    |         |            |          |    |
| long      | 6k    |         |            |          |    |
| very long | 10k   |         |            |          |    |

3. 동시성 테스트 결과

| Concurrency | 평균 Latency | 최대 Latency | 최소 Latency |
| ----------- | ---------- | ---------- | ---------- |
| 1           |            |            |            |
| 2           |            |            |            |
| 4           |            |            |            |
| 8           |            |            |            |

4. Chunked Prefill 비교

| 입력 길이 | A(ON) latency | B(OFF) latency | 속도 향상률 |
| ----- | ------------- | -------------- | ------ |
| 3k    |               |                |        |
| 6k    |               |                |        |
| 10k   |               |                |        |

5. 결론

어떤 옵션이 성능에 가장 큰 영향을 줬는지
GPU 메모리 한계
모델 크기 대비 throughput 적정치
chunked prefill의 효과 여부
max_num_batched_tokens 조정 필요성
권장 설정안

