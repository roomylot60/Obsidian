# BiTNET - BitNet b1.58 2B4T

## Concept

- 1.58 bit(=1-trit) 양자화를 활용하여 LLM 모델의 효율성과 성능을 극대화한 혁신적 모델
- 기존의 고정밀도 모델과 비교하여 메모리 사용량과 에너지 소비를 크게 줄이면서도 유사한 성능을 유지

### 1-trit quantunization & BitLinear Structure

- 1.58 bit 양자화: 모델의 가중치를 {-1, 0, +1}의 세 가지 값으로 제한하여, 곱셈 연산을 덧셈으로 대체하고 메모리 사용을 최소화(1-trit quantunization)
- BitLinear Layer: 전통적인 `nn.Linear` 레이어를 대체하여, 1.58bit 양자화된 가중치를 사용하는 구조로, 모델의 계산 효율성을 향상
- Activation function & Normalization: $ReLU^{2}$ 활성화 함수와 Sub-Layer Noramalization을 사용하여 학습 안정성과 성능 향상상

### 응용 분야

- Edge Computing
- Energy efficiency
- Realtime Service

### 기능 및 성능

- 모델 크기: 2B의 파라미터를 보유하면서, 모델 크기는 약 400MB
- 성능 비교: LLaMA 3.2 1B, Gemma3 1B 등과 비교하여, GSM8K, PIQA 등 다양한 벤치마크에서 우수한 성능을 보임
- 추론 속도 및 에너지 소비: x86 CPU에서 2.37 배에서 6.17배의 속도 향상과 71.9% 에서 82.2%의 에너지 소비 감소를 달성

### 구축 및 실행 방법

1. 필수 조건 설치:

- python >= 3.9
- cmake >= 3.22
- clang >= 18
- conda(recommanded)

2. 레포지토리 클론 및 환경 설정:

```bash
git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp
pip install -r requirements.txt
```

3. 모델 다운로드 및 환경 구성:

```bash
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s
```

4. 추론 실행:

- `run_inference.py` 스크립트를 사용하여, 입력 프롬프트에 대한 모델의 응답을 확인
