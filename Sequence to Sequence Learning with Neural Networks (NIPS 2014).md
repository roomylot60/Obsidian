[Original Paper Link](https://arxiv.org/abs/1409.3215)
https://brunch.co.kr/@jean/5

---
![](Attatched/Pasted%20image%2020240328004006.png)
(출처) https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice

---
### 0. Abstract

- *DNN 모델*들은 어려운 학습 과제들에 대해 훌륭한 성능을 보여주는 강력한 모델들이다. 
- 하지만 DNN이 대량의 레이블 학습 데이터에 대해서는 잘 작동하는 반면, *시퀀스 학습에 대한 매핑에는 사용되고 있지 않다*. 
- 해당 논문에서는 일반적인 end-to-end 접근으로 시퀀스 구조에 대한 최소한의 가정을 통한 시퀀스 학습을 제시한다. 우리의 방법은 *다층화 된 LSTM을 입력 시퀀스를 벡터로 매핑하는 데에 쓰고, 다른 LSTM을 타겟 시퀀스를 벡터로 부터 디코드 하는 데에 사용*한다. 
- 우리의 주요 결과물은 WMT'14 dataset을 사용한 영어를 프랑스어로 번역하는 작업이며, LSTM을 통한 해당 작업은 총 테스트 데이터에 대해 LSTM의 BLEU score가 사전 외 단어에 대해 패널티를 받고서 BLEU score 34.8 점을 달성했다. 
- 또한 LSTM은 장문에 대해서도 잘 작동했다. 
- 반면, 구문 기반의 SMT(Statistical Machine Translation) system이 같은 데이터에 대해 BLEU score 33.3을 달성했다. 
- 우리가 SMT system으로 생성한 LSTM을 1000개의 가설에 대해 재평가하였을 때, BLEU score는 36.5라는 이전의 최고 결과값에 근접한 결과를 얻었다. 
- LSTM은 문장 순서에 민감하고 상대적으로 능동태와 수동태에 불변하는 감각적 구문과 문장을 학습하였다. 
- 마지막으로 타겟 문장을 제외한 모든 문장 데이터의 단어의 순서를 거꾸로 하는 것이 입력 문장과 타겟 문장 사이의 단기 의존성을 도입하여 LSTM의 성능을 확연히 향상시키고 최적화 문제를 줄이는 것을 확인했다.

---
### 1. Introduction

- 심층 신경망은 매우 강력한 ML 모델로, 음성 인식과 시각적 물체 인식과 같은 어려운 문제에서 우수한 성능을 보여준다. 
- DNNs의 적정한 단계들로도 임의의 병렬 연산을 수행할 수 있기에 좋은 성능을 보여준다. 
- DNNs의 놀라운 특징 중 하나는 2차원 크기의 은닉 계층 2개를 사용해 N 비트 숫자를 정렬할 수 있다는 것이다. 
- 따라서 신경망은 전통적인 통계적 모델과 관련이 있음에도, 복잡한 연산을 학습한다. 
- 또한, 거대 DNNs는 레이블 훈련 데이터가 신경망의 매개변수를 명시하기에 충분한 정보를 가질 때, 지도 학습 역전파 과정이 이루어 질 수 있다. 따라서 거대 DNN에서 좋은 결과를 얻을 수 있는 매개변수 설정(인간이 빠르게 문제를 해결할 수 있어서)이 있을 때, 지도 학습 역전파는 이러한 매개변수를 찾아 문제를 해결한다.

- DNNs의 유연성과 능력에도 불구하고, DNNs는 입력과 타겟이 고정된 차원의 벡터로 인코딩될 수 있는 문제에만 적용될 수 있다. 
- 많은 중요한 문제들이 주로 사전에 길이를 알 수 없는 시퀀스로 표현되기 때문에 분명한 한계점이다. 
- 예를 들어, 음성 인식과 기계 번역은 순차적인 문제들니다. 마찬가지로, 질문 응대도 질문을 나타내는 단어 시퀀스에 대해 답변을 나타내는 단어 시퀀스로 매핑하는 것으로 볼 수 있다. 
- 따라서 시퀀스를 시퀀스로 매핑하는 도메인 독립적인 방법을 학습하는 것이 유용하다.

- DNNs가 고정되어 정해져 있는 입, 출력의 차원 수를 요구한다는 점에서 시퀀스는 DNNs에 도전 과제를 제시한다.
- 이 논문에서 LSTM 구조를 간단히 적용하면 일반적인 시퀀스에서 시퀀스로의 문제를 해결할 수 있음을 제시한다. 
- 이 아이디어는 하나의 LSTM을 사용하여 입력 시퀀스를 한 번에 한 시점씩 읽어내어 큰 고정 차원의 벡터 표현을 얻은 다음, 그 벡터에서 출력 시퀀스를 추출하기 위해 다른 LSTM을 사용하는 것이다.
- 두 번째 LSTM은 반드시 입력 시퀀스에 조건이 제시된 것을 제외한 RNN 언어 모델이다. 
- 장시간 의존성을 갖는 데이터에서 성공적으로 학습할 수 있는 LSTM의 능력은 입력과 그와 연동되는 출력 사이의 많은 지연 시간이 있기 때문에 이러한 응용에서 필수적이다.

- 신경망을 사용한 일반적인 시퀀스에서 시퀀스로의 학습 문제에 적용하기 위한 수많은 관련된 시도가 있었다.
- 우리의 접근 방식은 처음으로 전체 입력 시퀀스를 벡터로 매핑한 Kalchbrenner와 Blunsom의 방식과, 구문 기반 시스템으로 생성한 가설을 재평가 하는 데에만 사용한 Cho 등의 방식과 밀접한 관련이 있다.
- Graves는 새로운 신경망이 입력에 대해 서로 다른 부분에 집중하고 차별화 Attention 기법을 제시했고, 이 아이디어의 세련된 변형은 Bahdanau 등에 의한 기계 번역에 성공적으로 적용되었다.
- Connectionist Sequence Classification은 신경망을 사용하여 시퀀스를 시퀀스로 매핑하는 것에 대한 또 다른 유명한 기법이나, 입, 출력 사이의 단조적인 조정을 가정한다.
![](Attatched/Pasted%20image%2020240329005842.png)

  
- 이 연구의 주요 결과는 다음과 같다. WMT’14 영어에서 프랑스어 번역 작업에 대해, 단순히 왼쪽에서 오른쪽으로 진행하는 빔서치 디코더를 사용하는 5개의 심층 LSTM 앙상블(각각 384M 매개변수와 8,000 차원 상태값을 갖는)에서 직접 추출 번역을 통해 34.81의 BLEU score를 얻었다. 
- 이는 거대 신경망을 사용한 직접 번역으로 얻은 최고의 결과이다. 
- 반면, 동일 데이터 셋에서의 SMT 기반 모델의 BLEU score는 33.30이다. 
- 8만개의 단어를 갖는 어휘를 사용한 LSTM으로는 34.81의 BLEU score를 얻었고, 해당하는 8만개 외의 단어를 포함한 번역에서는 수치가 떨어졌다.
- 이 결과는 상대적으로 최적화되지 않은 작은 어휘 기반의 신경망 구조가 많은 개선 여지를 가지지만, 구문 기반 SMT 시스템보다 성능이 좋다는 것을 보여준다.

- 마지막으로, 우리는 동일 작업에서 SMT 기반의 공개적으로 사용가능한 1000개의 최고 리스트들을 재평가하는데 LSTM을 사용했다. 
- 그리하여, 우리는 해당 작업에서 최고 결과로 발표된 이전의 값 3.2 BLEU score점과 근접한 BLEU score 36.5를 얻었다.

- 놀랍게도 다른 연구가들의 관련된 구조들에 대한 최근의 연구 결과에도 불구하고 LSTM은 긴 문장에서도 잘 작동했다.
- 우리가 학습과 테스트 데이터에서 타겟 문장이 아닌 입력 문장의 순서를 뒤집어 사용했을 때 잘 작동했다.
- 그리하여, 우리는 최적화 문제들을 훨씬 간단하게 하는 수많은 단기 의존성을 제시한다.
- 결과적으로, SGD(Stochastic Gradient Descent; 확률적 경사 하강법)는 긴 문장에 대해 문제 없이 LSTMs를 학습할 수 있었다.
- 입력 문장의 단어 순서를 뒤집는 간단한 요소가 이번 결과의 기능적으로 기여하는 하나의 키가 되었다.

- LSTM의 유용한 특성은 입력 문장의 길이가 가변적일 때 고정 차원의 벡터 표현으로 매핑하는 것을 학습한다는 것이다. 
- 번역이 입력 문장에 대해 의역하는 경향이 있는 점을 고려하면, 번역 목표는 서로 비슷한 의미를 갖는 문장들은 가깝고, 다른 문장들은 멀어지도록 함으로써 LSTM이 그 의미를 포착하는 문장 표현을 찾는 것을 돕는다.
- 질적 평가는 이 주장의 근거로, 우리의 모델이 단어 순서를 인식하고, 능동태와 수동태에 대해 상당히 불변적인 것을 보여준다.

---
### 2. The model

- RNN; 순환 신경망은 FFNN의 시퀀스에 대한 일반화된 형태이다.
- 입력 시퀀스가 주어지면, 일반적인 RNN은 주어진 식의 반복을 통해 출력 시퀀스를 연산한다.
$$\begin{aligned}
&input\ sequence: (x_{1}, ..., x_{T})\\
&output\ sequence: (y_{1}, ..., y_{T})\\
&h_{t} = sigm(W^{hx}x_{t} + W^{hh}h_{t-1})\\
&y_{t} = W^{yh}h_{t}
\end{aligned}$$
- RNN은 입력과 출력 사이의 정렬에 대해 미리 알고 있을 경우 쉽게 시퀀스에서 시퀀스로 매핑할 수 있다. 
- 그러나 문제들의 입, 출력 시퀀스들이 단순하지 않은 관계와 복잡성과 다른 길이를 가질 때 ,RNN을 어떻게 해당 문제들에 적용 시킬 지는 불분명하다.
- 일반적인 시퀀스 학습의 가장 간단한 전략은 입력 시퀀스를 하나의 RNN를 사용하여 고정된 길이의 벡터로 매핑하고, 다른 RNN를 해당 벡터를 타겟 시퀀스로 매핑하는 데 사용하는 것이다.(이러한 접근법이 Cho 등에 의해 채용되었다.)
- 원칙적으로는 RNN이 관련된 모든 정보를 제공받기에 잘 작동하지만, 결과적으로는 장기 의존성을 야기하기 때문에 RNNs를 학습시키기에 어렵다.
- 그러나 LSTM은 장기 의존성 문제를 학습하는 것으로 알려져 있기에, LSTM은 이러한 설정에서 성공적으로 작동할 것이다.

- LSTM의 목표는 입, 출력 시퀀스가 각각 $x_{1},...,x_{T}, y_{1},...,y_{T^{'}}$일 때(각 도메인의 길이 $T$와 $T^{'}$가 다름), 조건부 확률 $p(y_{1},...,y_{T^{'}}|x_{1},...,x_{T})$를 평가하는 것이다.
- LSTM은 최초에 입력 시퀀스$(x_{1},...,x_{T})$에 대해 LSTM의 마지막 hidden state로 고정된 차원 표현 $v$를 얻는 이 조건부 확률을 연산하고, 초기 hidden state가 $x_{1},...,x_{T}$에 대한$v$인 표준 LSTM 언어모델 형식에 대해 $y_{1},...,y_{T^{'}}$의 조건부 확률을 연산한다.
$$p(y_{1},...,y_{T^{'}}|x_{1},...,x_{T}) = \Pi^{T^{'}}_{t=1}p(y_{t}|v,y_{1},...,y_{t-1})$$
- 이 식에서는 각 $p(y_{t}|v, y_{1},...,y_{t-1})$의 분포는 모든 사전 내의 단어들의 softmax로 표현된다.
- 우리는 Graves의 LSTM 형식을 사용한다. 각 문장이 특수한 문장 종료 심볼 `"<EOS>"`로 종료해야 하고, 그 모델이 모든 가능한 길이의 시퀀스에 대해 분포를 정의하도록 한다. 
- 전반적인 형태는 figure 1에 명시되어 있고 이미지에서 알 수 있듯 LSTM이 `"A", "B", "C". "<EOS>"`을 연산해 이를 사용하여 `"W", "X", "Y", "Z", "<EOS>"`의 확률을 계산한다.

- 실제 모델들은 위에서 언급한 내용과 3가지 주요 쟁점에서 차이가 있다.
- 첫번째로 우리는 두 개의 다른 LSTM을 사용했다: 하나는 입력 시퀀스에 다른 하나는 출력 시퀀스에 사용하는데, 이는 모델 파라미터를 무시할 수 있을 정도의 연산 비용만큼 증가하도록 하고, LSTM이 복수의 언어쌍에 대해 동시에 학습할 수 있도록 하기 때문이다.
- 두번째로 LSTM의 계층이 깊을 수록 성능이 확연히 올라가므로, 4계층의 LSTM을 사용했다.
- 세번째는 입력 문장의 단어 순서를 역전시키는 것이 확연히 성능을 향상시키는 점을 발견했다. 
- 예를 들어 $a, b, c$ 문장을 $\alpha, \beta, \gamma$ 로 매핑하는 것보다 LSTM에서는 $\alpha, \beta, \gamma$이 $a, b, c$의 번역일 때,  $c, b, a$를 $\alpha, \beta, \gamma$로 매핑하는 것이 요구된다.
- 이 방식은 $a$가 $\alpha$에 근접도를 갖고, $b$가 $\beta$에 근접하게 함으로써, 입력과 출력 사이의 "상호교류 수립"을 통해 SGD를 낮춰준다.
- 이 간단한 데이터 변환이 LSTM의 훌륭한 성능 향상을 이끄는 점을 알아내었다.
---
### 3. Experiments

- 위의 방법으로 WMT' 14 데이터로 영어에서 프랑스어로 MT(Machine Translation) 작업에 두 가지 방식으로 적용했다.
- 입력 시퀀스에 SMT system의 참조를 사용하지 않고 직접적으로 번역하는 것과 SMT 기반의 n-best 리스트를 재측정하는 데 사용했다.
- 우리는 토큰화 학습과 테스트 세트의 범용성 때문에 이 번역 작업과 특정한 학습용 데이터 셋을 선정했다

#### 3.1 Dataset details

우리는 WMT’14 영어에서 프랑스어 데이터셋을 사용했습니다. 348M 프랑스어 단어와 304M 영어 단어로 구성된 12M 문장의 하위 집합에서 모델을 훈련시켰습니다. 이는 [29]의 깨끗한 "선택된" 하위 집합입니다. 이 번역 작업과 특정 훈련 세트 하위 집합을 선택한 이유는 기본 SMT의 토큰화된 훈련 및 테스트 세트와 1000-최고 리스트가 공개적으로 사용 가능하기 때문입니다.

#### 3.2 Decoding and Rescoring