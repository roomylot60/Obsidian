[Original Paper Link](https://arxiv.org/abs/1409.3215)
https://brunch.co.kr/@jean/5

---
![](Attatched/Pasted%20image%2020240328004006.png)
(출처) https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice

---
## Abstract

- *DNN 모델*들은 어려운 학습 과제들에 대해 훌륭한 성능을 보여주는 강력한 모델들이다. 
- 하지만 DNN이 대량의 레이블 학습 데이터에 대해서는 잘 작동하는 반면, *시퀀스 학습에 대한 매핑에는 사용되고 있지 않다*. 
- 해당 논문에서는 일반적인 end-to-end 접근으로 시퀀스 구조에 대한 최소한의 가정을 통한 시퀀스 학습을 제시한다. 우리의 방법은 *다층화 된 LSTM을 입력 시퀀스를 벡터로 매핑하는 데에 쓰고, 다른 LSTM을 타겟 시퀀스를 벡터로 부터 디코드 하는 데에 사용*한다. 
- 우리의 주요 결과물은 WMT'14 dataset을 사용한 영어를 프랑스어로 번역하는 작업이며, LSTM을 통한 해당 작업은 총 테스트 데이터에 대해 LSTM의 BLEU score가 사전 외 단어에 대해 패널티를 받고서 BLEU score 34.8 점을 달성했다. 
- 또한 LSTM은 장문에 대해서도 잘 작동했다. 
- 반면, 구문 기반의 SMT(Statistical Machine Translation) system이 같은 데이터에 대해 BLEU score 33.3을 달성했다. 
- 우리가 SMT system으로 생성한 LSTM을 1000개의 가설에 대해 재평가하였을 때, BLEU score는 36.5라는 이전의 최고 결과값에 근접한 결과를 얻었다. 
- LSTM은 문장 순서에 민감하고 상대적으로 능동태와 수동태에 불변하는 감각적 구문과 문장을 학습하였다. 
- 마지막으로 타겟 문장을 제외한 모든 문장 데이터의 단어의 순서를 거꾸로 하는 것이 입력 문장과 타겟 문장 사이의 단기 의존성을 도입하여 LSTM의 성능을 확연히 향상시키고 최적화 문제를 줄이는 것을 확인했다.

---
## 1. Introduction

- 심층 신경망은 매우 강력한 ML 모델로, 음성 인식과 시각적 물체 인식과 같은 어려운 문제에서 우수한 성능을 보여준다. 
- DNNs의 적정한 단계들로도 임의의 병렬 연산을 수행할 수 있기에 좋은 성능을 보여준다. 
- DNNs의 놀라운 특징 중 하나는 2차원 크기의 은닉 계층 2개를 사용해 N 비트 숫자를 정렬할 수 있다는 것이다. 
- 따라서 신경망은 전통적인 통계적 모델과 관련이 있음에도, 복잡한 연산을 학습한다. 
- 또한, 거대 DNNs는 레이블 훈련 데이터가 신경망의 매개변수를 명시하기에 충분한 정보를 가질 때, 지도 학습 역전파 과정이 이루어 질 수 있다. 따라서 거대 DNN에서 좋은 결과를 얻을 수 있는 매개변수 설정(인간이 빠르게 문제를 해결할 수 있어서)이 있을 때, 지도 학습 역전파는 이러한 매개변수를 찾아 문제를 해결한다.

- DNNs의 유연성과 능력에도 불구하고, DNNs는 입력과 타겟이 고정된 차원의 벡터로 인코딩될 수 있는 문제에만 적용될 수 있다. 
- 많은 중요한 문제들이 주로 사전에 길이를 알 수 없는 시퀀스로 표현되기 때문에 분명한 한계점이다. 
- 예를 들어, 음성 인식과 기계 번역은 순차적인 문제들니다. 마찬가지로, 질문 응대도 질문을 나타내는 단어 시퀀스에 대해 답변을 나타내는 단어 시퀀스로 매핑하는 것으로 볼 수 있다. 
- 따라서 시퀀스를 시퀀스로 매핑하는 도메인 독립적인 방법을 학습하는 것이 유용하다.

- DNNs가 고정되어 정해져 있는 입, 출력의 차원 수를 요구한다는 점에서 시퀀스는 DNNs에 도전 과제를 제시한다.
- 이 논문에서 LSTM 구조를 간단히 적용하면 일반적인 시퀀스에서 시퀀스로의 문제를 해결할 수 있음을 제시한다. 
- 이 아이디어는 하나의 LSTM을 사용하여 입력 시퀀스를 한 번에 한 시점씩 읽어내어 큰 고정 차원의 벡터 표현을 얻은 다음, 그 벡터에서 출력 시퀀스를 추출하기 위해 다른 LSTM을 사용하는 것이다.
- 두 번째 LSTM은 반드시 입력 시퀀스에 조건이 제시된 것을 제외한 RNN 언어 모델이다. 
- 장시간 의존성을 갖는 데이터에서 성공적으로 학습할 수 있는 LSTM의 능력은 입력과 그와 연동되는 출력 사이의 많은 지연 시간이 있기 때문에 이러한 응용에서 필수적이다.

- 신경망을 사용한 일반적인 시퀀스에서 시퀀스로의 학습 문제에 적용하기 위한 수많은 관련된 시도가 있었다.
- 우리의 접근 방식은 처음으로 전체 입력 시퀀스를 벡터로 매핑한 Kalchbrenner와 Blunsom의 방식과, 구문 기반 시스템으로 생성한 가설을 재평가 하는 데에만 사용한 Cho 등의 방식과 밀접한 관련이 있다.
- Graves는 새로운 신경망이 입력에 대해 서로 다른 부분에 집중하고 차별화 Attention 기법을 제시했고, 이 아이디어의 세련된 변형은 Bahdanau 등에 의한 기계 번역에 성공적으로 적용되었다.
- Connectionist Sequence Classification은 신경망을 사용하여 시퀀스를 시퀀스로 매핑하는 것에 대한 또 다른 유명한 기법이나, 입, 출력 사이의 단조적인 조정을 가정한다.
![](Attatched/Pasted%20image%2020240329005842.png)

  
- 이 연구의 주요 결과는 다음과 같다. WMT’14 영어에서 프랑스어 번역 작업에 대해, 단순히 왼쪽에서 오른쪽으로 진행하는 빔서치 디코더를 사용하는 5개의 심층 LSTM 앙상블(각각 384M 매개변수와 8,000 차원 상태값을 갖는)에서 직접 추출 번역을 통해 34.81의 BLEU score를 얻었다. 
- 이는 거대 신경망을 사용한 직접 번역으로 얻은 최고의 결과이다. 
- 반면, 동일 데이터 셋에서의 SMT 기반 모델의 BLEU score는 33.30이다. 
- 8만개의 단어를 갖는 어휘를 사용한 LSTM으로는 34.81의 BLEU score를 얻었고, 해당하는 8만개 외의 단어를 포함한 번역에서는 수치가 떨어졌다.
- 이 결과는 상대적으로 최적화되지 않은 작은 어휘 기반의 신경망 구조가 많은 개선 여지를 가지지만, 구문 기반 SMT 시스템보다 성능이 좋다는 것을 보여준다.

- 마지막으로, 우리는 동일 작업에서 SMT 기반의 공개적으로 사용가능한 1000개의 최고 리스트들을 재평가하는데 LSTM을 사용했다. 
- 그리하여, 우리는 해당 작업에서 최고 결과로 발표된 이전의 값 3.2 BLEU score점과 근접한 BLEU score 36.5를 얻었다.

- 놀랍게도 다른 연구가들의 관련된 구조들에 대한 최근의 연구 결과에도 불구하고 LSTM은 긴 문장에서도 잘 작동했다.
- 우리가 학습과 테스트 데이터에서 타겟 문장이 아닌 입력 문장의 순서를 뒤집어 사용했을 때 잘 작동했다.
- 그리하여, 우리는 최적화 문제들을 훨씬 간단하게 하는 수많은 단기 의존성을 제시한다.
- 결과적으로, SGD(Stochastic Gradient Descent; 확률적 경사 하강법)는 긴 문장에 대해 문제 없이 LSTMs를 학습할 수 있었다.
- 입력 문장의 단어 순서를 뒤집는 간단한 요소가 이번 결과의 기능적으로 기여하는 하나의 키가 되었다.

- LSTM의 유용한 특성은 입력 문장의 길이가 가변적일 때 고정 차원의 벡터 표현으로 매핑하는 것을 학습한다는 것이다. 
- 번역이 입력 문장에 대해 의역하는 경향이 있는 점을 고려하면, 번역 목표는 서로 비슷한 의미를 갖는 문장들은 가깝고, 다른 문장들은 멀어지도록 함으로써 LSTM이 그 의미를 포착하는 문장 표현을 찾는 것을 돕는다.
- 질적 평가는 이 주장의 근거로, 우리의 모델이 단어 순서를 인식하고, 능동태와 수동태에 대해 상당히 불변적인 것을 보여준다.

---
## 2. The model

- RNN; 순환 신경망은 FFNN의 시퀀스에 대한 일반화된 형태이다.
- 입력 시퀀스가 주어지면, 일반적인 RNN은 주어진 식의 반복을 통해 출력 시퀀스를 연산한다.
$$\begin{aligned}
&input\ sequence: (x_{1}, ..., x_{T})\\
&output\ sequence: (y_{1}, ..., y_{T})\\
&h_{t} = sigm(W^{hx}x_{t} + W^{hh}h_{t-1})\\
&y_{t} = W^{yh}h_{t}
\end{aligned}$$
- RNN은 입력과 출력 사이의 정렬에 대해 미리 알고 있을 경우 쉽게 시퀀스에서 시퀀스로 매핑할 수 있다. 
- 그러나 문제들의 입, 출력 시퀀스들이 단순하지 않은 관계와 복잡성과 다른 길이를 가질 때 ,RNN을 어떻게 해당 문제들에 적용 시킬 지는 불분명하다.
- 일반적인 시퀀스 학습의 가장 간단한 전략은 입력 시퀀스를 하나의 RNN를 사용하여 고정된 길이의 벡터로 매핑하고, 다른 RNN를 해당 벡터를 타겟 시퀀스로 매핑하는 데 사용하는 것이다.(이러한 접근법이 Cho 등에 의해 채용되었다.)
- 원칙적으로는 RNN이 관련된 모든 정보를 제공받기에 잘 작동하지만, 결과적으로는 장기 의존성을 야기하기 때문에 RNNs를 학습시키기에 어렵다.
- 그러나 LSTM은 장기 의존성 문제를 학습하는 것으로 알려져 있기에, LSTM은 이러한 설정에서 성공적으로 작동할 것이다.

- LSTM의 목표는 입, 출력 시퀀스가 각각 $x_{1},...,x_{T}, y_{1},...,y_{T^{'}}$일 때(각 도메인의 길이 $T$와 $T^{'}$가 다름), 조건부 확률 $p(y_{1},...,y_{T^{'}}|x_{1},...,x_{T})$를 평가하는 것이다.
- LSTM은 최초에 입력 시퀀스$(x_{1},...,x_{T})$에 대해 LSTM의 마지막 hidden state로 고정된 차원 표현 $v$를 얻는 이 조건부 확률을 연산하고, 초기 hidden state가 $x_{1},...,x_{T}$에 대한$v$인 표준 LSTM 언어모델 형식에 대해 $y_{1},...,y_{T^{'}}$의 조건부 확률을 연산한다.
$$p(y_{1},...,y_{T^{'}}|x_{1},...,x_{T}) = \Pi^{T^{'}}_{t=1}p(y_{t}|v,y_{1},...,y_{t-1})$$
- 이 식에서는 각 $p(y_{t}|v, y_{1},...,y_{t-1})$의 분포는 모든 사전 내의 단어들의 softmax로 표현된다.
- 우리는 Graves의 LSTM 형식을 사용한다. 각 문장이 특수한 문장 종료 심볼 `"<EOS>"`로 종료해야 하고, 그 모델이 모든 가능한 길이의 시퀀스에 대해 분포를 정의하도록 한다. 
- 전반적인 형태는 figure 1에 명시되어 있고 이미지에서 알 수 있듯 LSTM이 `"A", "B", "C". "<EOS>"`을 연산해 이를 사용하여 `"W", "X", "Y", "Z", "<EOS>"`의 확률을 계산한다.

- 실제 모델들은 위에서 언급한 내용과 3가지 주요 쟁점에서 차이가 있다.
- 첫번째로 우리는 두 개의 다른 LSTM을 사용했다: 하나는 입력 시퀀스에 다른 하나는 출력 시퀀스에 사용하는데, 이는 모델 파라미터를 무시할 수 있을 정도의 연산 비용만큼 증가하도록 하고, LSTM이 복수의 언어쌍에 대해 동시에 학습할 수 있도록 하기 때문이다.
- 두번째로 LSTM의 계층이 깊을 수록 성능이 확연히 올라가므로, 4계층의 LSTM을 사용했다.
- 세번째는 입력 문장의 단어 순서를 역전시키는 것이 확연히 성능을 향상시키는 점을 발견했다. 
- 예를 들어 $a, b, c$ 문장을 $\alpha, \beta, \gamma$ 로 매핑하는 것보다 LSTM에서는 $\alpha, \beta, \gamma$이 $a, b, c$의 번역일 때,  $c, b, a$를 $\alpha, \beta, \gamma$로 매핑하는 것이 요구된다.
- 이 방식은 $a$가 $\alpha$에 근접도를 갖고, $b$가 $\beta$에 근접하게 함으로써, 입력과 출력 사이의 "상호교류 수립"을 통해 SGD를 낮춰준다.
- 이 간단한 데이터 변환이 LSTM의 훌륭한 성능 향상을 이끄는 점을 알아내었다.
---
## 3. Experiments

- 위의 방법으로 WMT' 14 데이터로 영어에서 프랑스어로 MT(Machine Translation) 작업에 두 가지 방식으로 적용했다.
- SMT system을 참조하지 않고 입력 시퀀스를 직접적으로 번역하는 것과 SMT 기반으로 n-best 리스트를 재측정하는 데 사용했다.
- 이러한 번역 방식과 기존의 샘플 번역들의 정확도를 측정하였고, 문장 표현의 결과를 시각화했다.

### 3.1 Dataset details

- 우리는 WMT’14 영어에서 프랑스어 데이터 셋을 사용했다. 
- Holger Schwenk의 논문에서 제시된 "선택된" 하위 집합인 348M 프랑스어 단어와 304M 영어 단어로 구성된 12M 문장으로 모델을 훈련했다.
- 이 번역 작업과 특정 훈련 세트 하위 집합을 선택한 이유는 위 논문에서의 SMT 기반 1000-best 리스트로 토큰화된 훈련 및 테스트 세트 모두 공개적 사용이 가능하기 때문입니다.
- 보편적인 신경망 언어 모델은 각 단에의 벡터 표현에 의존하기 때문에 우리는 두 언어에 대해 고정된 vocabulary;사전을 사용했다.
- 입력 언어에서 가장 많이 사용되는 단어 16만 개와, 타겟 언어에서의 단어 8만 개를 사용했다.
- 모든 사전 외 단어는 `"UNK"` 토큰으로 대체했다.
### 3.2 Decoding and Rescoring

- 해당 연구의 주요 쟁점은 수많은 문장 쌍에 대한 거대 심층 LSTM의 학습을 포함한다.
- 우리는 주어진 문장 $S$에 대한 올바른 번역 $T$의 log 확률을 최대화하여 학습하였고, 학습 객체는 $S$가 학습 데이터일 때 $1/|S| \sum_{T,S \in S}log\ p(T|S)$ 이다.
- 일단 학습이 완료되면, 우리는 LSTM에 따라 가장 적합한 번역을 수행했다: $\hat T = arg \ max_{T} \ p(T|S)$ 
- 우리는 일부 번역의 접두사인 부분 가설의 작은 수 $B$를 유지하는 간단한 순차 검색(left-to-right beam search) 디코더를 사용하여 가장 적합한 번역을 탐색했다. 
- 각 시점에서 탐색 범위 내의 각 부분 가설을 사전 내의 모든 가능한 단어로 확장했다. 
- 이로 인해 가설의 수가 크게 증가하므로, 모델의 로그 확률에 따라 $B$와 가장 가까운 가설을 제외한 나머지는 모두 제거했다. 
- `"<EOS>"` 심볼이 가설에 추가되면, 그 가설은 범위에서 제거되고 완전한 가설 집합에 추가된다. 
- 이 디코더는 대략적이지만, 구현하기 쉽다. 
- 흥미롭게도, 탐색 범위가 1일 때도 우리의 시스템이 잘 작동했고, 크기가 2일 때, 해당 검색 방식의 이점 대부분을 제공한다.

- 또한 기본 시스템에서 생성된 1000-best 리스트를 재평가하는 데 LSTM을 사용했다. 
- n-best 리스트를 재평가하기 위해, LSTM으로 모든 가설의 로그 확률을 계산하고 그 점수와 LSTM의 점수로 평균을 구했다.
### 3.3 Reversing the Source Sentences

- LSTM이 장기 의존성 문제를 해결하는 데 사용 가능한 반면, 우리는 LSTM이 Source Sentences가 역전되었을 때 훨씬 더 학습이 잘 이루어 지는 것을 발견했다(Target Sentences는 역전되지 않았다).
- 그렇게 함으로써, LSTM의 test perplexity가 5.8에서 4.7로 떨어지고, 디코딩된 번역의 test BLEU score는 25.9에서 30.6으로 증가했다.

- 우리가 이러한 현상의 실험을 완전하게 진행하지 못했으나, 우리는 이 현상이 dataset에 대한 수많은 단기 의존성의 도입으로 인한 것이라고 믿는다.
- 일반적으로, 우리가 source sentence를 target sentence와 병합했을 때, source sentence의 각 단어와 target sentence의 단어들의 연관성이 떨어졌다.
- 결과적으로 문제에서 많은 "minimal time lag; 최소 시간 지연"을 갖게 되었다.
- source sentence의 단어들을 뒤집음으로써, source와 target 언어에서 연관 단어들 간의 평균 거리가 변화하지 않았다.
- 그러나 source language의 최초 몇몇 단어들은 target language의 최초 몇몇 단어들과의 거리가 매우 가까워지고, 문제의 최소 시간 지연은 매우 감소하였다.
- 따라서, source sentence와 target sentence 간의 "establishing communication; 교류 수립"을 하면서 역전파가 쉬워졌고, 차례로 전반적으로 지속적인 성능 향상의 결과를 얻었다.

- 최초에 우리는 입력 문장들을 뒤집는 것이 target sentence의 초기 부분의 예측을 확고히 하고 후반부로 갈수록 예측률이 떨어질 것이라고 예상했다.
- 그러나 역전된 source sentences로 학습한 LSTMs은 일반 source sentences로 학습한 LSTM 보다 긴 문장들에 대해 훨씬 더 잘 작동하여, 입력 문장의 역전하는 것이 LSTMs이 메모리를 더 잘 이용한다는 것의 근거가 되었다.

### 3.4 Training details

- LSTM의 학습이 상당히 쉽다. 16만 개의 입력 단어와 8만 개의 출력 단어로 각 계층에 1000개의 cell과 1000개의 단어 embedding 차원을 갖는 4계층의 심층 LSTMs를 사용했다.
- 따라서 심층 LSTM은 8000개의 실수를 문장 표현에 사용한다.
- 각 추가 계층이 아마도 더 많은 hidden states; 은닉 상태로 인해 perplexity를 거의 10%씩 낮춤으로써 심층 LSTM이 확실히 저층 LSTM에 비해 성능이 뛰어나다는 것을 알아냈다.
- 우리는 각 결과에서 8만 개 이상의 단어들에 단순 softmax함수를 적용했다.
- 최종적으로 LSTM은 64백만 개는 순전히 recurrent connection인 (32백만 개는 "encoder", 32백만 개는 "decoder") 총 384백만 개의 parameter를 얻었다. 
- 최종 학습의 상세는 다음과 같다:
	- LSTM의 parameter들을 -0.08에서 0.08 사이의 균등 분포로 초기화했다.
	- 학습률은 0.7로 고정하고, momentum; 관성없이 SGD; 확률적 경사하강법을 적용하였다. 5번의 epoch 뒤부터, half epoch 마다 학습률을 절반으로 줄였다. 모델은 총 7.5 epoch만큼 훈련시켰다.
	- Gradient ;경사에 대해 128개 시퀀스의 batch를 수행하여 batch의 크기로 나누었다.
	- LSTM이 vanishing gradient; 경사 소멸 문제는 잘 일어나지 않으나, gradient explosion; 경사 폭발이 일어날 수 있다. 따라서 경사 표준이 설정한 한계점을 넘을 때마다 스케일링을 진행하도록 강제하였다. 각 학습 batch에서 우리는 128로 나눈 gradient $g$일 때, $s= \parallel g\parallel_{2}$ 를 연산했다. 만약 $s>5$ 일 때, $g=\frac {5g}{s}$로 설정했다.
	- 서로 다른 문장들은 다른 길이를 갖는다. 대부분의 문장들은 (20~30 정도로)짧으나, 몇몇 문장들은 (100을 넘을 정도로)길기에, 랜덤으로 고른 128개의 훈련 문장으로 minibatch에서도 긴 것은 거의 없고 주로 짧아, minibatch에서의 연산의 대부분이 낭비이다. 이러한 문제를 조정하기 위해, 우리는 minibatch의 모든 문장들을 일괄적으로 같은 길이를 갖도록 하여 두 배속으로 수행하였다.

### 3.5 Parallelization

- 심층 LSTM의 C++ 실행은 대략 초 당 1700개의 단어의 속도로 진행함으로써 단일 GPU에서의 이전과 비교된다. 
- 이는 우리의 목적에 비해 너무 느리므로, 우리는 모델에 8개의 GPU를 병렬 적용했다.
- LSTM의 각 층은 서로 다른 GPU에서 연산을 진행하고, 연산이 끝나자마자 그 결과를 다음 GPU / 계층과 통신하였다.
- 우리의 모델은 각 4계층이 분리된 GPU에 생성된 LSTM이다.
- 남은 4개의 GPU는 softmax를 병렬 처리하는 데 사용되어 각 GPU가 $1000\times 20000$행렬의 곱셈을 담당한다.
- 최종 실행은 크기 128의 minibatch에서 초 당 6300개의 단어(영어, 프랑스어 모두)의 속도를 달성했다.
- 학습은 해당 실행에서 약 열흘에 걸쳐 수행되었다.

### 3.6 Experimental Results
- BLEU score를 사용하여 번역을 평가했다.
- BLEU score를 토큰화 한 예측과 `multi-blue.pl`을 사용해 연산하였다.
![](Attatched/Pasted%20image%2020240330212619.png)

![](Attatched/Pasted%20image%2020240330212644.png)
### 3.7 Performance on long sentences

![](Attatched/Pasted%20image%2020240330212512.png)

![](Attatched/Pasted%20image%2020240330212323.png)

- 위의 그래프에서 수치적으로 볼 수 있듯이, LSTM이 긴 문장에 대해서 잘 동작한다는 놀라운 점을 알게 되었다.
- 위의 표는 몇몇 장문 예시와 그 번역에 대한 내용이다.
### 3.8 Model Analysis
![](Attatched/Pasted%20image%2020240330212554.png)
## 4. Related Work

---

## 5. Conclusion

---
